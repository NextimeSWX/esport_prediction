"""
Mod√®les ML avanc√©s pour maximiser la note du projet
√âcole89 - 2025
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import VotingClassifier, StackingClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
import warnings
warnings.filterwarnings('ignore')

class AdvancedCSGOModels:
    """Mod√®les ML avanc√©s pour la pr√©diction CS:GO"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.ensemble_models = {}
        self.results = {}
    
    def create_base_models(self):
        """Cr√©e une collection √©tendue de mod√®les de base"""
        
        self.models = {
            # Mod√®les lin√©aires
            'logistic_regression': LogisticRegression(
                random_state=self.random_state, max_iter=1000
            ),
            
            # Mod√®les bas√©s sur les arbres
            'random_forest': RandomForestClassifier(
                n_estimators=100, random_state=self.random_state, n_jobs=-1
            ),
            'extra_trees': ExtraTreesClassifier(
                n_estimators=100, random_state=self.random_state, n_jobs=-1
            ),
            'xgboost': xgb.XGBClassifier(
                random_state=self.random_state, eval_metric='logloss', verbosity=0
            ),
            'gradient_boosting': GradientBoostingClassifier(
                random_state=self.random_state, n_estimators=100
            ),
            
            # Mod√®les probabilistes
            'naive_bayes': GaussianNB(),
            
            # Support Vector Machine
            'svm_linear': SVC(
                kernel='linear', probability=True, random_state=self.random_state
            ),
            'svm_rbf': SVC(
                kernel='rbf', probability=True, random_state=self.random_state
            ),
            
            # R√©seau de neurones
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(100, 50), random_state=self.random_state,
                max_iter=500, early_stopping=True
            )
        }
        
        print(f"ü§ñ {len(self.models)} mod√®les de base cr√©√©s")
        return self.models
    
    def create_ensemble_models(self):
        """Cr√©e des mod√®les d'ensemble avanc√©s"""
        
        # Mod√®les de base pour les ensembles
        base_models = [
            ('rf', RandomForestClassifier(n_estimators=50, random_state=self.random_state)),
            ('xgb', xgb.XGBClassifier(n_estimators=50, random_state=self.random_state, verbosity=0)),
            ('gb', GradientBoostingClassifier(n_estimators=50, random_state=self.random_state)),
            ('lr', LogisticRegression(random_state=self.random_state))
        ]
        
        self.ensemble_models = {
            # Voting Classifier (Hard)
            'voting_hard': VotingClassifier(
                estimators=base_models,
                voting='hard'
            ),
            
            # Voting Classifier (Soft) - Utilise les probabilit√©s
            'voting_soft': VotingClassifier(
                estimators=base_models,
                voting='soft'
            ),
            
            # Stacking Classifier
            'stacking': StackingClassifier(
                estimators=base_models,
                final_estimator=LogisticRegression(random_state=self.random_state),
                cv=5
            )
        }
        
        print(f"üîó {len(self.ensemble_models)} mod√®les d'ensemble cr√©√©s")
        return self.ensemble_models
    
    def evaluate_all_models(self, X_train, y_train, cv_folds=5):
        """√âvalue tous les mod√®les avec validation crois√©e"""
        
        print("üìä √âvaluation de tous les mod√®les...")
        
        # Stratified K-Fold pour donn√©es d√©s√©quilibr√©es
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        all_models = {**self.models, **self.ensemble_models}
        
        for name, model in all_models.items():
            print(f"  √âvaluation: {name}")
            
            try:
                # Validation crois√©e multiple m√©triques
                accuracy_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                roc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')
                f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')
                
                self.results[name] = {
                    'accuracy_mean': accuracy_scores.mean(),
                    'accuracy_std': accuracy_scores.std(),
                    'roc_auc_mean': roc_scores.mean(),
                    'roc_auc_std': roc_scores.std(),
                    'f1_mean': f1_scores.mean(),
                    'f1_std': f1_scores.std(),
                    'model': model
                }
                
                print(f"    ‚úÖ AUC: {roc_scores.mean():.4f} (¬±{roc_scores.std():.4f})")
                
            except Exception as e:
                print(f"    ‚ùå Erreur: {e}")
                continue
        
        return self.results
    
    def get_model_ranking(self, metric='roc_auc_mean'):
        """Classe les mod√®les par performance"""
        
        if not self.results:
            raise ValueError("Lancez d'abord evaluate_all_models()")
        
        ranking = sorted(
            self.results.items(),
            key=lambda x: x[1][metric],
            reverse=True
        )
        
        print(f"\nüèÜ CLASSEMENT DES MOD√àLES ({metric}):")
        print("-" * 50)
        
        for i, (name, results) in enumerate(ranking, 1):
            score = results[metric]
            std = results.get(f"{metric.split('_')[0]}_std", 0)
            print(f"{i:2d}. {name:<20} {score:.4f} (¬±{std:.4f})")
        
        return ranking
    
    def create_meta_ensemble(self, top_n=3):
        """Cr√©e un ensemble des meilleurs mod√®les"""
        
        ranking = self.get_model_ranking()
        top_models = ranking[:top_n]
        
        # Pr√©parer les estimateurs pour le m√©ta-ensemble
        estimators = [
            (name, results['model']) 
            for name, results in top_models
        ]
        
        meta_ensemble = VotingClassifier(
            estimators=estimators,
            voting='soft'
        )
        
        print(f"\nüöÄ M√©ta-ensemble cr√©√© avec les {top_n} meilleurs mod√®les:")
        for name, _ in estimators:
            print(f"  - {name}")
        
        return meta_ensemble
    
    def perform_statistical_tests(self, X_train, y_train):
        """Effectue des tests statistiques entre mod√®les"""
        from scipy import stats
        
        print("\nüìà Tests statistiques de comparaison...")
        
        # Comparer les 3 meilleurs mod√®les
        ranking = self.get_model_ranking()
        top_3 = ranking[:3]
        
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        model_scores = {}
        for name, results in top_3:
            scores = cross_val_score(results['model'], X_train, y_train, cv=cv, scoring='roc_auc')
            model_scores[name] = scores
        
        # Tests de significativit√© (t-test pair√©)
        models = list(model_scores.keys())
        for i in range(len(models)):
            for j in range(i+1, len(models)):
                model1, model2 = models[i], models[j]
                scores1, scores2 = model_scores[model1], model_scores[model2]
                
                # T-test pair√©
                t_stat, p_value = stats.ttest_rel(scores1, scores2)
                
                diff = scores1.mean() - scores2.mean()
                significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else ""
                
                print(f"  {model1} vs {model2}:")
                print(f"    Diff√©rence: {diff:+.4f} (p={p_value:.4f}) {significance}")
        
        return model_scores
    
    def generate_model_report(self):
        """G√©n√®re un rapport complet des mod√®les"""
        
        print("\n" + "="*60)
        print("RAPPORT COMPLET DES MOD√àLES ML")
        print("="*60)
        
        if not self.results:
            print("‚ùå Aucun r√©sultat disponible")
            return
        
        # Statistiques g√©n√©rales
        n_models = len(self.results)
        best_model = max(self.results.items(), key=lambda x: x[1]['roc_auc_mean'])
        worst_model = min(self.results.items(), key=lambda x: x[1]['roc_auc_mean'])
        
        print(f"\nüìä STATISTIQUES G√âN√âRALES:")
        print(f"  Nombre de mod√®les test√©s: {n_models}")
        print(f"  Meilleur mod√®le: {best_model[0]} (AUC: {best_model[1]['roc_auc_mean']:.4f})")
        print(f"  Pire mod√®le: {worst_model[0]} (AUC: {worst_model[1]['roc_auc_mean']:.4f})")
        
        # Distribution des performances
        auc_scores = [r['roc_auc_mean'] for r in self.results.values()]
        print(f"  AUC moyen: {np.mean(auc_scores):.4f}")
        print(f"  √âcart-type AUC: {np.std(auc_scores):.4f}")
        
        # Cat√©gorisation des mod√®les
        excellent = [name for name, r in self.results.items() if r['roc_auc_mean'] > 0.9]
        good = [name for name, r in self.results.items() if 0.8 <= r['roc_auc_mean'] <= 0.9]
        moderate = [name for name, r in self.results.items() if r['roc_auc_mean'] < 0.8]
        
        print(f"\nüéØ CAT√âGORISATION DES PERFORMANCES:")
        print(f"  Excellents (AUC > 0.9): {len(excellent)} mod√®les")
        for model in excellent[:3]:  # Top 3
            print(f"    - {model}")
        
        print(f"  Bons (0.8 ‚â§ AUC ‚â§ 0.9): {len(good)} mod√®les")
        print(f"  Mod√©r√©s (AUC < 0.8): {len(moderate)} mod√®les")
        
        # Recommandations
        print(f"\nüí° RECOMMANDATIONS:")
        if len(excellent) >= 3:
            print("  ‚úÖ Plusieurs mod√®les excellents - Utiliser un ensemble")
        elif len(excellent) >= 1:
            print("  üëç Au moins un mod√®le excellent identifi√©")
        else:
            print("  ‚ö†Ô∏è Performances limit√©es - Revoir les features ou la target")
        
        if best_model[1]['roc_auc_std'] > 0.05:
            print("  ‚ö†Ô∏è Variance √©lev√©e - Augmenter les donn√©es ou la r√©gularisation")
        
        return {
            'n_models': n_models,
            'best_model': best_model[0],
            'best_score': best_model[1]['roc_auc_mean'],
            'auc_scores': auc_scores
        }

# Utilisation dans le pipeline principal
def run_advanced_modeling(X_train, y_train):
    """Lance l'analyse compl√®te avec mod√®les avanc√©s"""
    
    # Initialiser
    advanced_models = AdvancedCSGOModels()
    
    # Cr√©er tous les mod√®les
    advanced_models.create_base_models()
    advanced_models.create_ensemble_models()
    
    # √âvaluer
    results = advanced_models.evaluate_all_models(X_train, y_train)
    
    # Analyser
    ranking = advanced_models.get_model_ranking()
    stats_results = advanced_models.perform_statistical_tests(X_train, y_train)
    
    # Cr√©er m√©ta-ensemble
    meta_model = advanced_models.create_meta_ensemble(top_n=3)
    
    # Rapport final
    report = advanced_models.generate_model_report()
    
    return {
        'advanced_models': advanced_models,
        'results': results,
        'ranking': ranking,
        'meta_ensemble': meta_model,
        'report': report
    }

if __name__ == "__main__":
    print("üöÄ Test des mod√®les avanc√©s...")